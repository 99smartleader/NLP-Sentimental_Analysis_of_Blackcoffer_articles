{"cells":[{"cell_type":"markdown","source":["#  **Data Extraction**"],"metadata":{"id":"m8JX0uIfYzZU"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31704,"status":"ok","timestamp":1686331478165,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"},"user_tz":-330},"id":"5ApmqBtU_iiV","outputId":"d9e0bae1-13b3-480a-f99f-51045b15d1ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Title not found for URL_ID: 44\n","Title not found for URL_ID: 57\n","Title not found for URL_ID: 144\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","\n","# Read the Excel file\n","df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')\n","\n","# Iterate through each row and extract the article text\n","for index, row in df.iterrows():\n","    url_id = row['URL_ID']\n","    url = row['URL']\n","    \n","    # Create a session object for improved performance\n","    with requests.Session() as session:\n","        # Send a GET request to the URL\n","        response = session.get(url)\n","    \n","    # Parse the HTML content using BeautifulSoup\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    \n","    # Find the article title element\n","    title_element = soup.find('h1')\n","    if title_element:\n","        title = title_element.text\n","    else:\n","        print(f\"Title not found for URL_ID: {url_id}\")\n","        continue\n","    \n","    # Find the p tags as they contain the text element\n","    p_tags = soup.select('.tagdiv-type p')\n","    \n","    # Extract the text content of each <p> element using a list comprehension\n","    paragraphs = [p.get_text() for p in p_tags]\n","    \n","    # Check if any paragraphs were extracted\n","    if not paragraphs:\n","        print(f\"Article not found for URL_ID: {url_id}\")\n","        continue\n","    \n","    # Create a text file with the URL_ID as the file name and write the article content\n","    file_name = f'{url_id}.txt'\n","    with open(file_name, 'w') as file:\n","        # Write the title in bold using Unicode characters\n","        file.write(f'\\u2022\\u2022\\u2022\\u2022\\u2022 {title} \\u2022\\u2022\\u2022\\u2022\\u2022\\n\\n')\n","        \n","        # Write the article content\n","        file.write('\\n'.join(paragraphs))\n"]},{"cell_type":"markdown","source":["By further Research the three title with URL_ID = \"44, 57, 144\" which were not found because the page was removed.\n"],"metadata":{"id":"V_Rc4yM3Znhp"}},{"cell_type":"markdown","source":["# Data Analysis"],"metadata":{"id":"dYUxiv3YbRPA"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"-SYyqUnr_olp","executionInfo":{"status":"ok","timestamp":1686331478166,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"outputs":[],"source":["# Create a list of URL IDs where the title is not found\n","url_ids_to_remove = [44, 57, 144]\n","\n","# Filter the DataFrame to keep only the rows where the URL ID is not in the list\n","df = df[~df['URL_ID'].isin(url_ids_to_remove)]\n"]},{"cell_type":"code","source":["    import os\n","    # Define the path to the StopWords folder\n","    stopwords_folder = '/content/drive/MyDrive/20211030 Test Assignment/StopWords'\n","\n","    stop_words = set()\n","\n","    # Iterate through each file in the StopWords folder\n","    for file_name in os.listdir(stopwords_folder):\n","        # Read the content of the file\n","        with open(os.path.join(stopwords_folder, file_name), encoding='latin-1') as file:\n","           # Extract stopwords from each line in the file\n","           for line in file:\n","              # Remove any leading or trailing whitespace\n","              line = line.strip()\n","            \n","              # Check if the line contains \" | \"\n","              if \" | \" in line:\n","                # Split the line by \" | \" separator and take the first part\n","                line = line.split(\" | \")[0]\n","\n","              # Append the modified line to the stop_words set\n","              stop_words.add(line)\n"],"metadata":{"id":"FYCJsxKV_R8z","executionInfo":{"status":"ok","timestamp":1686331479126,"user_tz":-330,"elapsed":964,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BoapnNRE1NI4","executionInfo":{"status":"ok","timestamp":1686332784615,"user_tz":-330,"elapsed":480,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","import re\n","\n","results = []\n","# Iterate through each row and perform the analysis\n","for index, row in df.iterrows():\n","    url_id = row['URL_ID']\n","    url = row['URL']\n","    filename = f\"{url_id}.txt\"\n","    \n","    # Read the text from the file\n","    with open(filename, 'r') as file:\n","        text = file.read()\n"," \n","    # SENTIMENTAL ANALYSIS\n","    # Cleaning using Stop Words Lists\n","    cleaned_text = ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words])\n","\n","    # Load positive and negative dictionaries\n","    positive_words = set(open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/positive-words.txt', encoding='latin-1').read().splitlines())\n","    negative_words = set(open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/negative-words.txt', encoding='latin-1').read().splitlines())\n","\n","\n","    \n","    # Creating a dictionary of Positive and Negative words\n","    positive_score = sum(1 for word in word_tokenize(cleaned_text) if word.lower() in positive_words)\n","    negative_score = sum(1 for word in word_tokenize(cleaned_text) if word.lower() in negative_words)\n","    \n","    # Extracting Derived variables\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(word_tokenize(cleaned_text)) + 0.000001)\n","    \n","\n","    # Analysis of Readability\n","    sentences = sent_tokenize(text)\n","    word_count = sum(1 for word in word_tokenize(text) if re.match(r'^\\w+$', word))\n","    sentence_count = len(sentences)\n","    average_sentence_length = word_count / sentence_count\n","    \n","    complex_words = [word for word in word_tokenize(text) if len(word) > 2 and re.match(r'^\\w+$', word)]\n","    percentage_of_complex_words = len(complex_words) / word_count\n","    fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n","    \n","    # Average Words per Sentence\n","    average_words_per_sentence = word_count / sentence_count\n","    \n","    # Syllable in the article\n","    syllable_count = sum(1 for word in word_tokenize(text) if re.match(r'^\\w+$', word) and len(re.findall(r'[aeiouy]+', word.lower())) > 2 and not word.endswith(('es', 'ed')))\n","    \n","    # Personal Pronouns in the article\n","    personal_pronouns = sum(1 for word in word_tokenize(text) if word.lower() in ['i', 'we', 'my', 'ours', 'us'])\n","    \n","    # Average Word Length in the article\n","    average_word_length = sum(len(word) for word in word_tokenize(text)) / word_count\n","    \n","    # Store the analysis results in a dictionary\n","    analysis_result = {\n","        'URL_ID': url_id,\n","        'URL': url,\n","        'POSITIVE SCORE': positive_score,\n","        'NEGATIVE SCORE': negative_score,\n","        'POLARITY SCORE': polarity_score,\n","        'SUBJECTIVITY SCORE': subjectivity_score,\n","        'AVG SENTENCE LENGTH': average_sentence_length,\n","        'PERCENTAGE OF COMPLEX WORDS': percentage_of_complex_words,\n","        'FOG INDEX': fog_index,\n","        'AVG NUMBER OF WORDS PER SENTENCE': average_words_per_sentence,\n","        'COMPLEX WORD COUNT': len(complex_words),\n","        'WORD COUNT': word_count,\n","        'SYLLABLE PER WORD': syllable_count,\n","        'PERSONAL PRONOUNS': personal_pronouns,\n","        'AVG WORD LENGTH': average_word_length\n","    }\n","    \n","    # Append the result to the list\n","    results.append(analysis_result)\n","\n"],"metadata":{"id":"hQCWMb5rMivl","executionInfo":{"status":"ok","timestamp":1686331491195,"user_tz":-330,"elapsed":12072,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0378de21-0d7f-46b4-ca0f-f1bc0bf4f88d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Fa3Rje80Xe2k","executionInfo":{"status":"ok","timestamp":1686332773987,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Convert the results list into a DataFrame\n","output_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to an Excel file\n","output_df.to_excel('Output Data Structure.xlsx', index=False)"],"metadata":{"id":"LsQKWCFAKaSY","executionInfo":{"status":"ok","timestamp":1686331491195,"user_tz":-330,"elapsed":11,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KnkaRvGBKjBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YnBgyDTAK4zl","executionInfo":{"status":"ok","timestamp":1686334602640,"user_tz":-330,"elapsed":584,"user":{"displayName":"Amritesh Pandey","userId":"11051423802276354352"}}},"execution_count":12,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1ktxgZCcJE4mewCVBJGaPUsNCFsv0L8qu","authorship_tag":"ABX9TyM0Ac8+6Xcl1ZaGuN0cyEb6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}